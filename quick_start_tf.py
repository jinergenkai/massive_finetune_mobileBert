#!/usr/bin/env python3
# Quick Start Script - Fine-tune MobileBERT trÃªn MASSIVE Dataset
# Converted to TensorFlow from PyTorch

import os
import sys

from src.instruction import print_help
from src.util import show_intent_list, check_and_install_requirements
from src.data_processing import prepare_data

OUTPUT_DIR = "./models/mobilebert-massive-quick"
MAX_LENGTH = 128

def quick_train(language="en-US", epochs=2, batch_size=16):
    """
    Training nhanh vá»›i cáº¥u hÃ¬nh tá»‘i Æ°u
    
    Args:
        language: NgÃ´n ngá»¯ dataset (máº·c Ä‘á»‹nh: en-US)
        epochs: Sá»‘ epochs (máº·c Ä‘á»‹nh: 2 Ä‘á»ƒ train nhanh)
        batch_size: Batch size (máº·c Ä‘á»‹nh: 16)
    """
    print("="*60)
    print("ğŸš€ QUICK START - MobileBERT + MASSIVE Intent Classification (TensorFlow)")
    print("="*60)
    
    # Import sau khi Ä‘Ã£ cÃ i Ä‘áº·t packages
    import tensorflow as tf
    from datasets import load_dataset
    from transformers import (
        TFMobileBertForSequenceClassification,
        MobileBertTokenizer,
        create_optimizer
    )
    from sklearn.preprocessing import LabelEncoder
    from sklearn.metrics import accuracy_score
    import pandas as pd
    import numpy as np
    import json
    
    # Enable mixed precision for better performance
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    
    # Cáº¥u hÃ¬nh
    print(f"ğŸ“Š Sá»­ dá»¥ng ngÃ´n ngá»¯: {language}")
    print(f"â±ï¸  Sá»‘ epochs: {epochs}")
    print(f"ğŸ“¦ Batch size: {batch_size}")
    print(f"ğŸ’¾ Output directory: {OUTPUT_DIR}")
    
    # Check GPU
    print("GPU Available:", len(tf.config.experimental.list_physical_devices('GPU')) > 0)
    if len(tf.config.experimental.list_physical_devices('GPU')) == 0:
        print("   âš ï¸ Warning: GPU not available, training may be slow!")
    
    # Táº¡o thÆ° má»¥c output
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # 1. Load dataset
    print("\n1ï¸âƒ£ Äang load MASSIVE dataset...")
    try:
        dataset = load_dataset("AmazonScience/massive", language)
        print(f"   âœ“ Loaded successfully! Train: {len(dataset['train'])}, "
              f"Dev: {len(dataset['validation'])}, Test: {len(dataset['test'])}")
    except Exception as e:
        print(f"   âŒ Error loading dataset: {e}")
        return False
    
    # 2. Prepare data vÃ  táº¡o intent mapping
    print("\n2ï¸âƒ£ Äang chuáº©n bá»‹ dá»¯ liá»‡u...")
    train_df, dev_df, test_df, label_encoder = prepare_data(dataset)
    num_labels = len(label_encoder.classes_)
    
    # Láº¥y text intent tá»« ClassLabel cá»§a dataset
    intent_texts = dataset['train'].features['intent'].names  # danh sÃ¡ch text intent

    # Táº¡o mapping
    intent_mapping = {idx: intent_text for idx, intent_text in enumerate(intent_texts)}

    print(f"   âœ“ Sá»‘ intent classes: {len(intent_mapping)}")
    print(f"   âœ“ Created intent mapping: {len(intent_mapping)} intents")
    print(f"   âœ“ Sample intents: {list(intent_mapping.items())[:5]}")
    
    # 3. Initialize tokenizer and model
    print("\n3ï¸âƒ£ Äang khá»Ÿi táº¡o TensorFlow MobileBERT...")
    tokenizer = MobileBertTokenizer.from_pretrained("google/mobilebert-uncased")
    model = TFMobileBertForSequenceClassification.from_pretrained(
        "google/mobilebert-uncased",
        num_labels=num_labels
    )
    print("   âœ“ Model initialized!")
    
    # 4. Tokenize data vÃ  táº¡o TensorFlow datasets
    print("\n4ï¸âƒ£ Äang tokenize dá»¯ liá»‡u vÃ  táº¡o TF datasets...")
    
    def tokenize_function(examples):
        return tokenizer(
            examples['utt'],
            padding='max_length',
            truncation=True,
            max_length=MAX_LENGTH,
            return_tensors='tf'
        )
    
    # Prepare datasets
    def prepare_tf_dataset(hf_dataset, df, batch_size, is_training=False):
        # Extract texts and labels
        texts = hf_dataset['utt']
        intents = hf_dataset['intent']
        
        # Convert texts to list of strings
        texts = [str(t) for t in texts]
        
        # Create label mapping
        label_encoder_dict = dict(zip(df['intent'], df['label']))
        labels = [label_encoder_dict[intent] for intent in intents]
        
        # Tokenize all texts
        encoded = tokenizer(
            texts,
            padding='max_length',
            truncation=True,
            max_length=MAX_LENGTH,
            return_tensors='tf'
        )
        
        # Create TensorFlow dataset
        tf_dataset = tf.data.Dataset.from_tensor_slices({
            'input_ids': encoded['input_ids'],
            'attention_mask': encoded['attention_mask'],
            'token_type_ids': encoded.get('token_type_ids', tf.zeros_like(encoded['input_ids'])),  # Some tokenizers may not return token_type_ids
            'labels': tf.constant(labels)
        })
        
        if is_training:
            tf_dataset = tf_dataset.shuffle(1000)
        
        tf_dataset = tf_dataset.batch(batch_size)
        tf_dataset = tf_dataset.prefetch(tf.data.AUTOTUNE)
        
        return tf_dataset

    def prepare_tf_dataset_old(hf_dataset, df, batch_size, is_training=False):
        # Tokenize
        texts = hf_dataset['utt']
        intents = hf_dataset['intent']
        
        # Create label mapping
        label_encoder_dict = dict(zip(df['intent'], df['label']))
        labels = [label_encoder_dict[intent] for intent in intents]
        
        # Tokenize all texts
        encoded = tokenizer(
            texts,
            padding='max_length',
            truncation=True,
            max_length=MAX_LENGTH,
            return_tensors='tf'
        )
        
        # Create TensorFlow dataset
        tf_dataset = tf.data.Dataset.from_tensor_slices({
            'input_ids': encoded['input_ids'],
            'attention_mask': encoded['attention_mask'],
            'token_type_ids': encoded['token_type_ids'],
            'labels': tf.constant(labels)
        })
        
        if is_training:
            tf_dataset = tf_dataset.shuffle(1000)
        
        tf_dataset = tf_dataset.batch(batch_size)
        tf_dataset = tf_dataset.prefetch(tf.data.AUTOTUNE)
        
        return tf_dataset
    
    train_dataset = prepare_tf_dataset(dataset['train'], train_df, batch_size, is_training=True)
    dev_dataset = prepare_tf_dataset(dataset['validation'], dev_df, batch_size)
    test_dataset = prepare_tf_dataset(dataset['test'], test_df, batch_size)
    
    print("   âœ“ TensorFlow datasets created!")
    
    # 5. Setup training
    print("\n5ï¸âƒ£ Äang setup training...")
    
    # Calculate training steps
    train_steps = len(dataset['train']) // batch_size
    total_steps = train_steps * epochs
    warmup_steps = min(1000, total_steps // 10)
    
    # Create optimizer
    optimizer, schedule = create_optimizer(
        init_lr=5e-5,
        num_warmup_steps=warmup_steps,
        num_train_steps=total_steps
    )
    
    # Compile model
    model.compile(
        optimizer=optimizer,
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=['accuracy']
    )
    
    print("   âœ“ Training setup completed!")
    
    # 6. Setup callbacks
    print("\n6ï¸âƒ£ Äang setup callbacks...")
    
    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(
            filepath=os.path.join(OUTPUT_DIR, 'best_model'),
            monitor='val_accuracy',
            save_best_only=True,
            save_weights_only=False,
            mode='max',
            verbose=1
        ),
        tf.keras.callbacks.EarlyStopping(
            monitor='val_accuracy',
            patience=2,
            mode='max',
            restore_best_weights=True,
            verbose=1
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_accuracy',
            factor=0.5,
            patience=1,
            mode='max',
            verbose=1
        )
    ]
    
    # 7. Start training
    print("\n7ï¸âƒ£ Báº¯t Ä‘áº§u training...")
    print("   (CÃ³ thá»ƒ máº¥t vÃ i phÃºt tÃ¹y thuá»™c vÃ o hardware...)")
    
    try:
        history = model.fit(
            train_dataset,
            validation_data=dev_dataset,
            epochs=epochs,
            callbacks=callbacks,
            verbose=1
        )
        print("   âœ“ Training completed successfully!")
    except Exception as e:
        print(f"   âŒ Training error: {e}")
        return False
    
    # 8. Evaluate on test set
    print("\n8ï¸âƒ£ ÄÃ¡nh giÃ¡ model...")
    
    test_results = model.evaluate(test_dataset, verbose=0)
    test_accuracy = test_results[1]  # accuracy metric
    
    print(f"   âœ“ Test Accuracy: {test_accuracy:.4f}")
    
    # 9. Save model
    print("\n9ï¸âƒ£ LÆ°u model...")
    
    # Save the model
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    # Save label encoder vÃ  intent mapping
    import joblib
    joblib.dump(label_encoder, os.path.join(OUTPUT_DIR, 'label_encoder.pkl'))
    
    # Save intent mapping as JSON for easy loading
    with open(os.path.join(OUTPUT_DIR, 'intent_mapping.json'), 'w', encoding='utf-8') as f:
        json.dump(intent_mapping, f, indent=2, ensure_ascii=False)
    
    # Save training history
    with open(os.path.join(OUTPUT_DIR, 'training_history.json'), 'w') as f:
        json.dump(history.history, f, indent=2)
    
    print(f"   âœ“ Model saved to: {OUTPUT_DIR}")
    print(f"   âœ“ Intent mapping saved to: {OUTPUT_DIR}/intent_mapping.json")
    
    # 10. Quick test
    print("\nğŸ”Ÿ Quick test vá»›i má»™t sá»‘ samples...")
    
    test_samples = [
        "set alarm for 7 am tomorrow",
        "play some music", 
        "what's the weather",
        "order pizza",
        "turn off lights"
    ]
    
    for sample in test_samples:
        inputs = tokenizer(
            sample,
            padding='max_length',
            truncation=True,
            max_length=MAX_LENGTH,
            return_tensors='tf'
        )
        
        # Predict
        outputs = model(**inputs)
        predictions = tf.nn.softmax(outputs.logits, axis=-1)
        predicted_id = tf.argmax(predictions, axis=-1)[0].numpy()
        confidence = predictions[0][predicted_id].numpy()
        
        # Use intent mapping to get text label
        predicted_intent = intent_mapping.get(predicted_id, f"Unknown_ID_{predicted_id}")
        
        print(f"   ğŸ“ '{sample}' -> {predicted_intent} ({confidence:.3f}) | Label ID: {predicted_id}")
    
    print("\n" + "="*60)
    print("ğŸ‰ HOÃ€N THÃ€NH!")
    print(f"âœ… Model Ä‘Ã£ Ä‘Æ°á»£c train vÃ  lÆ°u táº¡i: {OUTPUT_DIR}")
    print(f"âœ… Test accuracy: {test_accuracy:.4f}")
    print("âœ… CÃ³ thá»ƒ sá»­ dá»¥ng model cho inference!")
    print("="*60)
    
    return True

def quick_inference_demo(model_path=OUTPUT_DIR):
    """Demo nhanh inference"""
    print("\n" + "="*50)
    print("ğŸ”® QUICK INFERENCE DEMO (TensorFlow)")
    print("="*50)
    
    if not os.path.exists(model_path):
        print(f"âŒ Model khÃ´ng tÃ¬m tháº¥y táº¡i: {model_path}")
        print("   HÃ£y cháº¡y training trÆ°á»›c!")
        return False
    
    try:
        import tensorflow as tf
        import joblib
        import json
        from transformers import TFMobileBertForSequenceClassification, MobileBertTokenizer
        
        # Load model
        print("ğŸ”„ Äang load model...")
        model = TFMobileBertForSequenceClassification.from_pretrained(model_path)
        tokenizer = MobileBertTokenizer.from_pretrained(model_path)
        
        # Load intent mapping
        intent_mapping_path = os.path.join(model_path, 'intent_mapping.json')
        if os.path.exists(intent_mapping_path):
            with open(intent_mapping_path, 'r', encoding='utf-8') as f:
                intent_mapping = json.load(f)
                # Convert string keys to int
                intent_mapping = {int(k): v for k, v in intent_mapping.items()}
            print(f"   âœ… Loaded intent mapping: {len(intent_mapping)} intents")
        else:
            # Fallback to label encoder (old method)
            label_encoder = joblib.load(os.path.join(model_path, 'label_encoder.pkl'))
            intent_mapping = {idx: intent for idx, intent in enumerate(label_encoder.classes_)}
            print("   âš ï¸ Using fallback label encoder mapping")
        
        print("âœ… Model loaded successfully!")
        
        # Show some example intents
        print(f"\nğŸ“‹ Available intents (showing first 10):")
        for i, intent in list(intent_mapping.items())[:10]:
            print(f"   {i}: {intent}")
        if len(intent_mapping) > 10:
            print(f"   ... vÃ  {len(intent_mapping)-10} intents khÃ¡c")
        
        # Interactive demo
        print("\nğŸ’¬ Interactive mode (gÃµ 'quit' Ä‘á»ƒ thoÃ¡t):")
        
        while True:
            text = input("\nğŸ¯ Nháº­p cÃ¢u cáº§n phÃ¢n loáº¡i: ").strip()
            
            if text.lower() == 'quit':
                break
                
            if not text:
                continue
            
            # Predict
            inputs = tokenizer(
                text,
                padding='max_length',
                truncation=True,
                max_length=128,
                return_tensors='tf'
            )
            
            outputs = model(**inputs)
            probabilities = tf.nn.softmax(outputs.logits, axis=-1)
            
            # Top 3 predictions
            top_probs, top_indices = tf.nn.top_k(probabilities[0], k=3)
            
            print(f"\nğŸ“Š Káº¿t quáº£ cho: '{text}'")
            print("-" * 50)
            for i in range(3):
                index = top_indices[i].numpy()
                intent_text = intent_mapping.get(index, f"Unknown_ID_{index}")
                confidence = top_probs[i].numpy()
                print(f"   {i+1}. {intent_text} ({confidence:.4f}) [ID: {index}]")
        
        print("ğŸ‘‹ Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng!")
        return True
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Main function"""
    print("ğŸ¤– MobileBERT + MASSIVE Dataset Fine-tuning (TensorFlow)")
    print("=" * 60)
    
    # Parse arguments
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == "train":
            # Training mode
            language = sys.argv[2] if len(sys.argv) > 2 else "en-US"
            epochs = int(sys.argv[3]) if len(sys.argv) > 3 else 2
            
            print(f"ğŸ‹ï¸ Training mode - Language: {language}, Epochs: {epochs}")
            
            # Check requirements
            if check_and_install_requirements():
                quick_train(language=language, epochs=epochs)
            
        elif command == "inference":
            # Inference mode
            model_path = sys.argv[2] if len(sys.argv) > 2 else OUTPUT_DIR
            quick_inference_demo(model_path)
            
        elif command == "intents":
            # Show intent list
            model_path = sys.argv[2] if len(sys.argv) > 2 else OUTPUT_DIR
            show_intent_list(model_path)
            
        elif command == "help":
            print_help()
            
        else:
            print(f"âŒ Unknown command: {command}")
            print_help()
    else:
        # Default: training
        print("ğŸ‹ï¸ Default mode: Training")
        
        # Check requirements
        if check_and_install_requirements():
            success = quick_train()
            
            if success:
                print("\nğŸ¯ Báº¡n cÃ³ muá»‘n test inference khÃ´ng? (y/n): ", end="")
                choice = input().strip().lower()
                
                if choice == 'y':
                    quick_inference_demo()

if __name__ == "__main__":
    main()