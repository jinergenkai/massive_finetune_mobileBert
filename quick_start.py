#!/usr/bin/env python3
# Quick Start Script - Fine-tune MobileBERT trÃªn MASSIVE Dataset
# Fixed: Correct intent label mapping

import os
import sys

from src.instruction import print_help
from src.util import show_intent_list, check_and_install_requirements
from src.data_processing import prepare_data

OUTPUT_DIR = "./models/mobilebert-massive-quick"
MAX_LENGTH = 128

def quick_train(language="en-US", epochs=2, batch_size=16):
    """
    Training nhanh vá»›i cáº¥u hÃ¬nh tá»‘i Æ°u
    
    Args:
        language: NgÃ´n ngá»¯ dataset (máº·c Ä‘á»‹nh: en-US)
        epochs: Sá»‘ epochs (máº·c Ä‘á»‹nh: 2 Ä‘á»ƒ train nhanh)
        batch_size: Batch size (máº·c Ä‘á»‹nh: 16)
    """
    print("="*60)
    print("ğŸš€ QUICK START - MobileBERT + MASSIVE Intent Classification")
    print("="*60)
    
    # Import sau khi Ä‘Ã£ cÃ i Ä‘áº·t packages
    import torch
    from datasets import load_dataset
    from transformers import (
        MobileBertTokenizer, 
        MobileBertForSequenceClassification,
        TrainingArguments, 
        Trainer,
        DataCollatorWithPadding
    )
    from sklearn.preprocessing import LabelEncoder
    from sklearn.metrics import accuracy_score
    import pandas as pd
    import numpy as np
    import json
    
    # Cáº¥u hÃ¬nh
    
    print(f"ğŸ“Š Sá»­ dá»¥ng ngÃ´n ngá»¯: {language}")
    print(f"â±ï¸  Sá»‘ epochs: {epochs}")
    print(f"ğŸ“¦ Batch size: {batch_size}")
    print(f"ğŸ’¾ Output directory: {OUTPUT_DIR}")
    
    # Táº¡o thÆ° má»¥c output
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # 1. Load dataset
    print("\n1ï¸âƒ£ Äang load MASSIVE dataset...")
    try:
        dataset = load_dataset("AmazonScience/massive", language)
        print(f"   âœ“ Loaded successfully! Train: {len(dataset['train'])}, "
              f"Dev: {len(dataset['validation'])}, Test: {len(dataset['test'])}")
    except Exception as e:
        print(f"   âŒ Error loading dataset: {e}")
        return False
    
    # 2. Prepare data vÃ  táº¡o intent mapping
    print("\n2ï¸âƒ£ Äang chuáº©n bá»‹ dá»¯ liá»‡u...")
    train_df, dev_df, test_df, label_encoder = prepare_data(dataset)
    num_labels = len(label_encoder.classes_)
    
    # Láº¥y text intent tá»« ClassLabel cá»§a dataset
    intent_texts = dataset['train'].features['intent'].names  # danh sÃ¡ch text intent

    # Táº¡o mapping
    intent_mapping = {idx: intent_text for idx, intent_text in enumerate(intent_texts)}

    print(f"   âœ“ Sá»‘ intent classes: {len(intent_mapping)}")
    print(f"   âœ“ Created intent mapping: {len(intent_mapping)} intents")
    print(f"   âœ“ Sample intents: {list(intent_mapping.items())[:5]}")
    
    # 3. Initialize tokenizer and model
    print("\n3ï¸âƒ£ Äang khá»Ÿi táº¡o MobileBERT...")
    tokenizer = MobileBertTokenizer.from_pretrained("google/mobilebert-uncased")
    model = MobileBertForSequenceClassification.from_pretrained(
        "google/mobilebert-uncased",
        num_labels=num_labels
    )
    print("   âœ“ Model initialized!")
    
    # 4. Tokenize data
    print("\n4ï¸âƒ£ Äang tokenize dá»¯ liá»‡u...")
    
    def tokenize_function(examples):
        return tokenizer(
            examples['utt'],
            padding='max_length',
            truncation=True,
            max_length=MAX_LENGTH
        )
    
    # Apply tokenization
    train_dataset = dataset['train'].map(tokenize_function, batched=True)
    dev_dataset = dataset['validation'].map(tokenize_function, batched=True)
    test_dataset = dataset['test'].map(tokenize_function, batched=True)
    
    # Add labels
    def add_labels(examples, df):
        label_encoder_dict = dict(zip(df['intent'], df['label']))
        examples['labels'] = [label_encoder_dict[intent] for intent in examples['intent']]
        return examples
    
    train_dataset = train_dataset.map(lambda x: add_labels(x, train_df), batched=True)
    dev_dataset = dev_dataset.map(lambda x: add_labels(x, dev_df), batched=True)
    test_dataset = test_dataset.map(lambda x: add_labels(x, test_df), batched=True)
    
    print("   âœ“ Tokenization completed!")
    
    # 5. Setup training
    print("\n5ï¸âƒ£ Äang setup training...")

    print("CUDA available:", torch.cuda.is_available())
    print("Model device:", next(model.parameters()).device)
    if not torch.cuda.is_available():
        print("   âš ï¸ Warning: CUDA not available, training may be slow!")
        return
    
    def compute_metrics(eval_pred):
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        accuracy = accuracy_score(labels, predictions)
        return {'accuracy': accuracy}
    
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        eval_strategy="epoch",
        save_strategy="epoch",
        logging_strategy="epoch",
        num_train_epochs=epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        warmup_steps=100,
        weight_decay=0.01,
        learning_rate=5e-5,
        logging_dir=f'{OUTPUT_DIR}/logs',
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",
        greater_is_better=True,
        save_total_limit=1,
        report_to=None,
        dataloader_num_workers=2,
    )
    
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=dev_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )
    
    print("   âœ“ Training setup completed!")
    
    # 6. Start training
    print("\n6ï¸âƒ£ Báº¯t Ä‘áº§u training...")
    print("   (CÃ³ thá»ƒ máº¥t vÃ i phÃºt tÃ¹y thuá»™c vÃ o hardware...)")
    
    try:
        trainer.train()
        print("   âœ“ Training completed successfully!")
    except Exception as e:
        print(f"   âŒ Training error: {e}")
        return False
    
    # 7. Evaluate
    print("\n7ï¸âƒ£ ÄÃ¡nh giÃ¡ model...")
    
    # Evaluate on test set
    test_results = trainer.evaluate(test_dataset)
    test_accuracy = test_results['eval_accuracy']
    
    print(f"   âœ“ Test Accuracy: {test_accuracy:.4f}")
    
    # 8. Save model
    print("\n8ï¸âƒ£ LÆ°u model...")
    
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    # Save label encoder vÃ  intent mapping
    import joblib
    joblib.dump(label_encoder, os.path.join(OUTPUT_DIR, 'label_encoder.pkl'))
    
    # Save intent mapping as JSON for easy loading
    with open(os.path.join(OUTPUT_DIR, 'intent_mapping.json'), 'w', encoding='utf-8') as f:
        json.dump(intent_mapping, f, indent=2, ensure_ascii=False)
    
    print(f"   âœ“ Model saved to: {OUTPUT_DIR}")
    print(f"   âœ“ Intent mapping saved to: {OUTPUT_DIR}/intent_mapping.json")
    
    # 9. Quick test
    print("\n9ï¸âƒ£ Quick test vá»›i má»™t sá»‘ samples...")
    
    test_samples = [
        "set alarm for 7 am tomorrow",
        "play some music", 
        "what's the weather",
        "order pizza",
        "turn off lights"
    ]
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    model.eval()
    
    for sample in test_samples:
        inputs = tokenizer(
            sample,
            padding='max_length',
            truncation=True,
            max_length=MAX_LENGTH,
            return_tensors='pt'
        ).to(device)
        
        with torch.no_grad():
            outputs = model(**inputs)
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
            predicted_id = torch.argmax(predictions, dim=-1).item()
            confidence = predictions[0][predicted_id].item()
        
        # Use intent mapping to get text label
        predicted_intent = intent_mapping.get(predicted_id, f"Unknown_ID_{predicted_id}")
        
        print(f"   ğŸ“ '{sample}' -> {predicted_intent} ({confidence:.3f}) | Label ID: {predicted_id}")
    
    print("\n" + "="*60)
    print("ğŸ‰ HOÃ€N THÃ€NH!")
    print(f"âœ… Model Ä‘Ã£ Ä‘Æ°á»£c train vÃ  lÆ°u táº¡i: {OUTPUT_DIR}")
    print(f"âœ… Test accuracy: {test_accuracy:.4f}")
    print("âœ… CÃ³ thá»ƒ sá»­ dá»¥ng model cho inference!")
    print("="*60)
    
    return True

def quick_inference_demo(model_path=OUTPUT_DIR):
    """Demo nhanh inference"""
    print("\n" + "="*50)
    print("ğŸ”® QUICK INFERENCE DEMO")
    print("="*50)
    
    if not os.path.exists(model_path):
        print(f"âŒ Model khÃ´ng tÃ¬m tháº¥y táº¡i: {model_path}")
        print("   HÃ£y cháº¡y training trÆ°á»›c!")
        return False
    
    try:
        import torch
        import joblib
        import json
        from transformers import MobileBertTokenizer, MobileBertForSequenceClassification
        
        # Load model
        print("ğŸ”„ Äang load model...")
        model = MobileBertForSequenceClassification.from_pretrained(model_path)
        tokenizer = MobileBertTokenizer.from_pretrained(model_path)
        
        # Load intent mapping
        intent_mapping_path = os.path.join(model_path, 'intent_mapping.json')
        if os.path.exists(intent_mapping_path):
            with open(intent_mapping_path, 'r', encoding='utf-8') as f:
                intent_mapping = json.load(f)
                # Convert string keys to int
                intent_mapping = {int(k): v for k, v in intent_mapping.items()}
            print(f"   âœ… Loaded intent mapping: {len(intent_mapping)} intents")
        else:
            # Fallback to label encoder (old method)
            label_encoder = joblib.load(os.path.join(model_path, 'label_encoder.pkl'))
            intent_mapping = {idx: intent for idx, intent in enumerate(label_encoder.classes_)}
            print("   âš ï¸ Using fallback label encoder mapping")
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)
        model.eval()
        
        print("âœ… Model loaded successfully!")
        
        # Show some example intents
        print(f"\nğŸ“‹ Available intents (showing first 10):")
        for i, intent in list(intent_mapping.items())[:10]:
            print(f"   {i}: {intent}")
        if len(intent_mapping) > 10:
            print(f"   ... vÃ  {len(intent_mapping)-10} intents khÃ¡c")
        
        # Interactive demo
        print("\nğŸ’¬ Interactive mode (gÃµ 'quit' Ä‘á»ƒ thoÃ¡t):")
        
        while True:
            text = input("\nğŸ¯ Nháº­p cÃ¢u cáº§n phÃ¢n loáº¡i: ").strip()
            
            if text.lower() == 'quit':
                break
                
            if not text:
                continue
            
            # Predict
            inputs = tokenizer(
                text,
                padding='max_length',
                truncation=True,
                max_length=128,
                return_tensors='pt'
            ).to(device)
            
            with torch.no_grad():
                outputs = model(**inputs)
                probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
                
                # Top 3 predictions
                top_probs, top_indices = torch.topk(probabilities[0], 3)
                
                print(f"\nğŸ“Š Káº¿t quáº£ cho: '{text}'")
                print("-" * 50)
                for i in range(3):
                    index = top_indices[i].item()
                    intent_text = intent_mapping.get(index, f"Unknown_ID_{index}")
                    confidence = top_probs[i].item()
                    print(f"   {i+1}. {intent_text} ({confidence:.4f}) [ID: {index}]")
        
        print("ğŸ‘‹ Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng!")
        return True
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Main function"""
    print("ğŸ¤– MobileBERT + MASSIVE Dataset Fine-tuning")
    print("=" * 60)
    
    # Parse arguments
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == "train":
            # Training mode
            language = sys.argv[2] if len(sys.argv) > 2 else "en-US"
            epochs = int(sys.argv[3]) if len(sys.argv) > 3 else 2
            
            print(f"ğŸ‹ï¸ Training mode - Language: {language}, Epochs: {epochs}")
            
            # Check requirements
            if check_and_install_requirements():
                quick_train(language=language, epochs=epochs)
            
        elif command == "inference":
            # Inference mode
            model_path = sys.argv[2] if len(sys.argv) > 2 else OUTPUT_DIR
            quick_inference_demo(model_path)
            
        elif command == "intents":
            # Show intent list
            model_path = sys.argv[2] if len(sys.argv) > 2 else OUTPUT_DIR
            show_intent_list(model_path)
            
        elif command == "help":
            print_help()
            
        else:
            print(f"âŒ Unknown command: {command}")
            print_help()
    else:
        # Default: training
        print("ğŸ‹ï¸ Default mode: Training")
        
        # Check requirements
        if check_and_install_requirements():
            success = quick_train()
            
            if success:
                print("\nğŸ¯ Báº¡n cÃ³ muá»‘n test inference khÃ´ng? (y/n): ", end="")
                choice = input().strip().lower()
                
                if choice == 'y':
                    quick_inference_demo()

if __name__ == "__main__":
    main()